# -*- coding: utf-8 -*-
"""xgboost-demo-heart-data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_LXmTxKgXoGr_4sxBCdG9RGFi9K0nypi
"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Dataset: https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %timeit df = pd.read_csv('heart_2020_cleaned.csv')

pip install modin[all]

# Commented out IPython magic to ensure Python compatibility.
import modin.pandas as pd
import os
from distributed import Client
client = Client()
os.environ["MODIN_ENGINE"] = "dask"
# %timeit pd.read_csv('heart_2020_cleaned.csv')

df.shape

df.head(5)

df['HeartDisease'].value_counts()

df['HeartDisease'] = df['HeartDisease'].map({'Yes': 1, 'No': 0})

cat_cols = df.select_dtypes(include='object').columns.tolist()
df = pd.get_dummies(df, columns=cat_cols)

X, y = df.drop('HeartDisease', axis=1), df['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

xg_reg = xgb.XGBRegressor(objective ='binary:hinge', 
                          eval_metric='logloss',
                          colsample_bytree = 0.3, 
                          learning_rate = 0.1,
                          max_depth = 5, 
                          alpha = 10, 
                          n_estimators = 10)

xg_reg.fit(X_train, y_train)
preds = xg_reg.predict(X_test)

print("Accuracy:",accuracy_score(y_test, preds))